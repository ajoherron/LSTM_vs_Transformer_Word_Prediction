{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from transformers import T5Config\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in model\n",
    "model = tf.keras.models.load_model('lstm_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input sequences:  41193\n"
     ]
    }
   ],
   "source": [
    "# Read in data, clean data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "train_data['title'] = train_data['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\n",
    "train_data['title'] = train_data['title'].apply(lambda x: x.replace('\\u200a',' '))\n",
    "\n",
    "# Set tokenizer for words not in vocab\n",
    "tokenizer = Tokenizer(oov_token='<oov>')\n",
    "tokenizer.fit_on_texts(train_data['title'])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create list of sequences\n",
    "input_sequences = []\n",
    "for line in train_data['title']:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]    \n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Convert labels to binary predictions\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "# Print number of sequences\n",
    "num_sequences = len(input_sequences)\n",
    "print(\"Total input sequences: \", num_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Predict Top Word for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_predict(input_string):\n",
    "    \n",
    "    token_list = tokenizer.texts_to_sequences([input_string])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted=model.predict(token_list) \n",
    "    predicted=np.argmax(predicted,axis=1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    input_string += \" \" + output_word\n",
    "        \n",
    "    return output_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_predict(\"Computational cognitive modeling is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Predict Top K Words for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_k(input_string, k):\n",
    "    \n",
    "    # Subtract 1 from k (account for not double counting first word)\n",
    "    k -= 1\n",
    "    \n",
    "    # Calculate probabilities for top k words\n",
    "    token_list = tokenizer.texts_to_sequences([input_string])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    prob = model.predict(token_list)\n",
    "    predicted = np.argmax(prob,axis=1)    \n",
    "    top_k_preds = np.argpartition(prob, k)\n",
    "    top_k_preds = top_k_preds[0][:k]\n",
    "    \n",
    "    # Predict most likely word\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            top_word = word\n",
    "    \n",
    "    # Predict next k-1 words\n",
    "    top_k_words = []\n",
    "    for i in range(k):\n",
    "        pred_i = top_k_preds[i]\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == pred_i:\n",
    "                top_k_words.append(word)\n",
    "    \n",
    "    # Print results\n",
    "    #print(f'Best prediction: {top_word}')\n",
    "    #print(f'Next {k} best predictions: {top_k_words}')\n",
    "    \n",
    "    # Add best result to list of words\n",
    "    top_k_words.insert(0, top_word)\n",
    "    return top_k_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'overthinking',\n",
       " 'ensure',\n",
       " 'eat',\n",
       " 'keyword',\n",
       " 'surveillance',\n",
       " 'responses',\n",
       " 'decide',\n",
       " 'teach',\n",
       " 'medical']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_top_k(\"Computational cognitive modeling is\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"allenai/t5-small-next-word-generator-qoogle\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Predict Top Word for Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_prediction(input_string, **generator_args):\n",
    "    input_ids = t5_tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    res = t5_model.generate(input_ids, **generator_args)\n",
    "    output = t5_tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'typically'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_prediction(\"Computational cognitive modeling projects are\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Evaluate Models on All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test data\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "test_titles = test_data['title']\n",
    "\n",
    "# Collect list of sequences and predicted words\n",
    "x_list = []\n",
    "y_list = []\n",
    "for i in range(len(test_titles)):    \n",
    "    text_i = test_titles.iloc[i].split(' ')\n",
    "    seq_i = text_i[:-1]\n",
    "    \n",
    "    x_i = ''    \n",
    "    for j in seq_i:\n",
    "        x_i += j + ' '\n",
    "    x_list.append(x_i)\n",
    "    \n",
    "    y_i = text_i[-1]\n",
    "    y_list.append(y_i)\n",
    "    \n",
    "# Collect model predictions\n",
    "lstm_preds = []\n",
    "trans_preds = []\n",
    "lstm_counter = 0\n",
    "trans_counter = 0\n",
    "for i in range(len(x_list)):\n",
    "\n",
    "    # Make lowercase\n",
    "    x_i = x_list[i].lower()\n",
    "    y_i = y_list[i].lower()\n",
    "    \n",
    "    # Get number of correct LSTM predictions\n",
    "    lstm_pred = lstm_predict(x_i)\n",
    "    lstm_preds.append(lstm_pred)\n",
    "    if lstm_pred == y_i:\n",
    "        lstm_counter += 1\n",
    "    \n",
    "    # Get number of correct Transformer predictions\n",
    "    trans_pred = transformer_prediction(x_i)\n",
    "    trans_preds.append(trans_pred)\n",
    "    if trans_pred == y_i:\n",
    "        trans_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Accuracy: 5.63525%\n",
      "Transformer Accuracy: 8.60656%\n",
      "\n",
      "LSTM Precision: 0.00891\n",
      "Transformer Precision: 0.01976\n",
      "\n",
      "LSTM Recall: 0.0123\n",
      "Transformer Recall: 0.02254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy, precision, recall, and categorical cross entropy\n",
    "num_decs = 5\n",
    "avg_setting = 'weighted'\n",
    "lstm_accuracy = round((lstm_counter / len(x_list)) * 100, num_decs)\n",
    "trans_accuracy = round((trans_counter / len(x_list)) * 100, num_decs)\n",
    "lstm_precision = round(precision_score(y_list, lstm_preds, average=avg_setting), num_decs)\n",
    "trans_precision = round(precision_score(y_list, trans_preds, average=avg_setting), num_decs)\n",
    "lstm_recall = round(recall_score(y_list, lstm_preds, average=avg_setting), num_decs)\n",
    "trans_recall = round(recall_score(y_list, trans_preds, average=avg_setting), num_decs)\n",
    "\n",
    "# Print results\n",
    "print(f'LSTM Accuracy: {lstm_accuracy}%')\n",
    "print(f'Transformer Accuracy: {trans_accuracy}%\\n')\n",
    "print(f'LSTM Precision: {lstm_precision}')\n",
    "print(f'Transformer Precision: {trans_precision}\\n')\n",
    "print(f'LSTM Recall: {lstm_recall}')\n",
    "print(f'Transformer Recall: {trans_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 42),\n",
       " ('and', 24),\n",
       " ('a', 20),\n",
       " ('in', 18),\n",
       " ('you', 14),\n",
       " ('your', 14),\n",
       " ('strong', 12),\n",
       " ('with', 10),\n",
       " ('to', 9),\n",
       " ('learning', 9)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_counter = Counter(lstm_preds)\n",
    "lstm_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 120),\n",
       " ('is', 94),\n",
       " ('the', 34),\n",
       " ('of', 16),\n",
       " ('learning', 15),\n",
       " ('and', 15),\n",
       " ('how', 14),\n",
       " ('are', 13),\n",
       " ('work', 10),\n",
       " ('to', 10)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_counter = Counter(trans_preds)\n",
    "trans_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Create Subset of Human Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick subset of 10 articles\n",
    "num_human_sequences = 10\n",
    "subset = test_data.sample(num_human_sequences)\n",
    "subset_titles = subset['title']\n",
    "\n",
    "# Print each title\n",
    "for title in subset_titles:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_titles = ['How Small beat Big',\n",
    "                 'How to foster a culture of customer obsession',\n",
    "                 'PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection',\n",
    "                 'UX candidates are asking the wrong questions',\n",
    "                 'Smart Farming In Agriculture Sector',\n",
    "                 'Rosenblatt’s perceptron, the very first neural network',\n",
    "                 'Adaptive Normalization and Fuzzy Targets — Time Series Forecasting tricks',\n",
    "                 'Startup India and Job Creation',\n",
    "                 'New Software: When and How to Implement into Your Business',\n",
    "                 'My Analysis from 50+ papers on the Application of ML in Credit Lending']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect list of sequences and predicted words\n",
    "x_list = []\n",
    "y_list = []\n",
    "for i in subset_titles:\n",
    "#for i in range(len(subset_titles)):  \n",
    "    text_i = i.split(' ')\n",
    "    #text_i = subset_titles.iloc[i].split(' ')\n",
    "    seq_i = text_i[:-1]\n",
    "\n",
    "    x_i = ''    \n",
    "    for j in seq_i:\n",
    "        x_i += j + ' '\n",
    "    x_list.append(x_i)\n",
    "    \n",
    "    y_i = text_i[-1]\n",
    "    y_list.append(y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How Small beat ',\n",
       " 'How to foster a culture of customer ',\n",
       " 'PVANET: Deep but Lightweight Neural Networks for Real-time Object ',\n",
       " 'UX candidates are asking the wrong ',\n",
       " 'Smart Farming In Agriculture ',\n",
       " 'Rosenblatt’s perceptron, the very first neural ',\n",
       " 'Adaptive Normalization and Fuzzy Targets — Time Series Forecasting ',\n",
       " 'Startup India and Job ',\n",
       " 'New Software: When and How to Implement into Your ',\n",
       " 'My Analysis from 50+ papers on the Application of ML in Credit ']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Question 1-----\n",
      "Sequence: how small beat \n",
      "Target: big\n",
      "LSTM prediction: big\n",
      "Transformer prediction: a\n",
      "\n",
      "-----Question 2-----\n",
      "Sequence: how to foster a culture of customer \n",
      "Target: obsession\n",
      "LSTM prediction: obsession\n",
      "Transformer prediction: service\n",
      "\n",
      "-----Question 3-----\n",
      "Sequence: pvanet: deep but lightweight neural networks for real-time object \n",
      "Target: detection\n",
      "LSTM prediction: detection\n",
      "Transformer prediction: oriented\n",
      "\n",
      "-----Question 4-----\n",
      "Sequence: ux candidates are asking the wrong \n",
      "Target: questions\n",
      "LSTM prediction: questions\n",
      "Transformer prediction: answer\n",
      "\n",
      "-----Question 5-----\n",
      "Sequence: smart farming in agriculture \n",
      "Target: sector\n",
      "LSTM prediction: sector\n",
      "Transformer prediction: is\n",
      "\n",
      "-----Question 6-----\n",
      "Sequence: rosenblatt’s perceptron, the very first neural \n",
      "Target: network\n",
      "LSTM prediction: network\n",
      "Transformer prediction: network\n",
      "\n",
      "-----Question 7-----\n",
      "Sequence: adaptive normalization and fuzzy targets — time series forecasting \n",
      "Target: tricks\n",
      "LSTM prediction: money\n",
      "Transformer prediction: are\n",
      "\n",
      "-----Question 8-----\n",
      "Sequence: startup india and job \n",
      "Target: creation\n",
      "LSTM prediction: creation\n",
      "Transformer prediction: interview\n",
      "\n",
      "-----Question 9-----\n",
      "Sequence: new software: when and how to implement into your \n",
      "Target: business\n",
      "LSTM prediction: business\n",
      "Transformer prediction: project\n",
      "\n",
      "-----Question 10-----\n",
      "Sequence: my analysis from 50+ papers on the application of ml in credit \n",
      "Target: lending\n",
      "LSTM prediction: lending\n",
      "Transformer prediction: report\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gather model predictions for subset\n",
    "lstm_counter = 0\n",
    "trans_counter = 0\n",
    "lstm_preds = []\n",
    "trans_preds = []\n",
    "for i in range(len(x_list)):\n",
    "    \n",
    "    print(f'-----Question {i+1}-----')\n",
    "    \n",
    "    # Collect correct targets for given sequence\n",
    "    x_i = x_list[i].lower()\n",
    "    y_i = y_list[i].lower()\n",
    "    \n",
    "    print(f'Sequence: {x_i}')\n",
    "    print(f'Target: {y_i}')\n",
    "    \n",
    "    # Make LSTM prediction\n",
    "    lstm_pred = lstm_predict(x_i)\n",
    "    lstm_preds.append(lstm_pred)\n",
    "    if lstm_pred == y_i:\n",
    "        lstm_counter += 1\n",
    "        \n",
    "    print(f'LSTM prediction: {lstm_pred}')\n",
    "    \n",
    "    # Make Transfromer prediction\n",
    "    trans_pred = transformer_prediction(x_i)\n",
    "    trans_preds.append(trans_pred)\n",
    "    if trans_pred == y_i:\n",
    "        trans_counter += 1\n",
    "        \n",
    "    print(f'Transformer prediction: {trans_pred}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Accuracy: 90.0%\n",
      "Transformer Accuracy: 10.0%\n",
      "\n",
      "LSTM Precision: 0.3\n",
      "Transformer Precision: 0.1\n",
      "\n",
      "LSTM Recall: 0.3\n",
      "Transformer Recall: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy, precision, recall, and categorical cross entropy\n",
    "num_decs = 5\n",
    "avg_setting = 'weighted'\n",
    "lstm_accuracy = round((lstm_counter / len(x_list)) * 100, num_decs)\n",
    "trans_accuracy = round((trans_counter / len(x_list)) * 100, num_decs)\n",
    "lstm_precision = round(precision_score(y_list, lstm_preds, average=avg_setting), num_decs)\n",
    "trans_precision = round(precision_score(y_list, trans_preds, average=avg_setting), num_decs)\n",
    "lstm_recall = round(recall_score(y_list, lstm_preds, average=avg_setting), num_decs)\n",
    "trans_recall = round(recall_score(y_list, trans_preds, average=avg_setting), num_decs)\n",
    "\n",
    "# Print results\n",
    "print(f'LSTM Accuracy: {lstm_accuracy}%')\n",
    "print(f'Transformer Accuracy: {trans_accuracy}%\\n')\n",
    "print(f'LSTM Precision: {lstm_precision}')\n",
    "print(f'Transformer Precision: {trans_precision}\\n')\n",
    "print(f'LSTM Recall: {lstm_recall}')\n",
    "print(f'Transformer Recall: {trans_recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Human Sequence Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of options for human participants to choose from\n",
    "k = 10\n",
    "subset_options = []\n",
    "for i in range(len(subset_titles)):\n",
    "    x_i = x_list[i]\n",
    "    y_i = y_list[i]\n",
    "    lstm_top_10 = predict_top_k(x_i, k)\n",
    "    \n",
    "    # Correct prediction\n",
    "    if y_i in lstm_top_10:\n",
    "        random.shuffle(lstm_top_10)            \n",
    "        subset_options.append(lstm_top_10)\n",
    "    \n",
    "    # Incorrect prediction\n",
    "    else:\n",
    "        lstm_top_10 = predict_top_k(x_i, k-1)\n",
    "        lstm_top_10.append(y_i)\n",
    "        random.shuffle(lstm_top_10)\n",
    "        subset_options.append(lstm_top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>choices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How Small beat Big</td>\n",
       "      <td>How Small beat</td>\n",
       "      <td>Big</td>\n",
       "      <td>[big, spice, managing, glorified, unclog, put,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to foster a culture of customer obsession</td>\n",
       "      <td>How to foster a culture of customer</td>\n",
       "      <td>obsession</td>\n",
       "      <td>[answer, reinventing, breaking, inspire, obses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PVANET: Deep but Lightweight Neural Networks f...</td>\n",
       "      <td>PVANET: Deep but Lightweight Neural Networks f...</td>\n",
       "      <td>Detection</td>\n",
       "      <td>[compete, sustainability, inflation, rescue, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UX candidates are asking the wrong questions</td>\n",
       "      <td>UX candidates are asking the wrong</td>\n",
       "      <td>questions</td>\n",
       "      <td>[quitting, features, questions, comparison, im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smart Farming In Agriculture Sector</td>\n",
       "      <td>Smart Farming In Agriculture</td>\n",
       "      <td>Sector</td>\n",
       "      <td>[accomplish, staff, Sector, dirty, hadoop, fak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rosenblatt’s perceptron, the very first neural...</td>\n",
       "      <td>Rosenblatt’s perceptron, the very first neural</td>\n",
       "      <td>network</td>\n",
       "      <td>[quantitative, hadoop, spring, bottom, buy, ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adaptive Normalization and Fuzzy Targets — Tim...</td>\n",
       "      <td>Adaptive Normalization and Fuzzy Targets — Tim...</td>\n",
       "      <td>tricks</td>\n",
       "      <td>[meetings, compete, surveillance, mvp, nearest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Startup India and Job Creation</td>\n",
       "      <td>Startup India and Job</td>\n",
       "      <td>Creation</td>\n",
       "      <td>[supply, storytelling, breath, perks, japan, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>New Software: When and How to Implement into Y...</td>\n",
       "      <td>New Software: When and How to Implement into Y...</td>\n",
       "      <td>Business</td>\n",
       "      <td>[business, controversy, damn, Business, key, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>My Analysis from 50+ papers on the Application...</td>\n",
       "      <td>My Analysis from 50+ papers on the Application...</td>\n",
       "      <td>Lending</td>\n",
       "      <td>[founders, fashion, those, lending, body, alte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0                                 How Small beat Big   \n",
       "1      How to foster a culture of customer obsession   \n",
       "2  PVANET: Deep but Lightweight Neural Networks f...   \n",
       "3       UX candidates are asking the wrong questions   \n",
       "4                Smart Farming In Agriculture Sector   \n",
       "5  Rosenblatt’s perceptron, the very first neural...   \n",
       "6  Adaptive Normalization and Fuzzy Targets — Tim...   \n",
       "7                     Startup India and Job Creation   \n",
       "8  New Software: When and How to Implement into Y...   \n",
       "9  My Analysis from 50+ papers on the Application...   \n",
       "\n",
       "                                            sequence     target  \\\n",
       "0                                    How Small beat         Big   \n",
       "1               How to foster a culture of customer   obsession   \n",
       "2  PVANET: Deep but Lightweight Neural Networks f...  Detection   \n",
       "3                UX candidates are asking the wrong   questions   \n",
       "4                      Smart Farming In Agriculture      Sector   \n",
       "5    Rosenblatt’s perceptron, the very first neural     network   \n",
       "6  Adaptive Normalization and Fuzzy Targets — Tim...     tricks   \n",
       "7                             Startup India and Job    Creation   \n",
       "8  New Software: When and How to Implement into Y...   Business   \n",
       "9  My Analysis from 50+ papers on the Application...    Lending   \n",
       "\n",
       "                                             choices  \n",
       "0  [big, spice, managing, glorified, unclog, put,...  \n",
       "1  [answer, reinventing, breaking, inspire, obses...  \n",
       "2  [compete, sustainability, inflation, rescue, c...  \n",
       "3  [quitting, features, questions, comparison, im...  \n",
       "4  [accomplish, staff, Sector, dirty, hadoop, fak...  \n",
       "5  [quantitative, hadoop, spring, bottom, buy, ef...  \n",
       "6  [meetings, compete, surveillance, mvp, nearest...  \n",
       "7  [supply, storytelling, breath, perks, japan, c...  \n",
       "8  [business, controversy, damn, Business, key, s...  \n",
       "9  [founders, fashion, those, lending, body, alte...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save as dataframe\n",
    "df_choices = pd.DataFrame(subset_titles)\n",
    "df_choices['sequence'] = x_list\n",
    "df_choices['target'] = y_list\n",
    "df_choices['choices'] = subset_options\n",
    "df_choices.to_csv('human_sequence_options.csv')\n",
    "df_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target 0: Big\n",
      "Choices:\n",
      "big\n",
      "spice\n",
      "managing\n",
      "glorified\n",
      "unclog\n",
      "put\n",
      "difference\n",
      "promotion\n",
      "Big\n",
      "finish\n",
      "\n",
      "Target 1: obsession\n",
      "Choices:\n",
      "answer\n",
      "reinventing\n",
      "breaking\n",
      "inspire\n",
      "obsession\n",
      "managing\n",
      "extraordinary\n",
      "nginx\n",
      "radically\n",
      "keras\n",
      "\n",
      "Target 2: Detection\n",
      "Choices:\n",
      "compete\n",
      "sustainability\n",
      "inflation\n",
      "rescue\n",
      "childhood\n",
      "catch\n",
      "stuck\n",
      "medical\n",
      "Detection\n",
      "detection\n",
      "\n",
      "Target 3: questions\n",
      "Choices:\n",
      "quitting\n",
      "features\n",
      "questions\n",
      "comparison\n",
      "improving\n",
      "leaving\n",
      "action\n",
      "node\n",
      "i’m\n",
      "ask\n",
      "\n",
      "Target 4: Sector\n",
      "Choices:\n",
      "accomplish\n",
      "staff\n",
      "Sector\n",
      "dirty\n",
      "hadoop\n",
      "fake\n",
      "said\n",
      "sector\n",
      "exploratory\n",
      "studying\n",
      "\n",
      "Target 5: network\n",
      "Choices:\n",
      "quantitative\n",
      "hadoop\n",
      "spring\n",
      "bottom\n",
      "buy\n",
      "efficient\n",
      "12\n",
      "morning\n",
      "higher\n",
      "network\n",
      "\n",
      "Target 6: tricks\n",
      "Choices:\n",
      "meetings\n",
      "compete\n",
      "surveillance\n",
      "mvp\n",
      "nearest\n",
      "money\n",
      "biggest\n",
      "share\n",
      "tricks\n",
      "level\n",
      "\n",
      "Target 7: Creation\n",
      "Choices:\n",
      "supply\n",
      "storytelling\n",
      "breath\n",
      "perks\n",
      "japan\n",
      "creation\n",
      "improving\n",
      "launching\n",
      "shit\n",
      "Creation\n",
      "\n",
      "Target 8: Business\n",
      "Choices:\n",
      "business\n",
      "controversy\n",
      "damn\n",
      "Business\n",
      "key\n",
      "shouldn’t\n",
      "supply\n",
      "sure\n",
      "walt\n",
      "overview\n",
      "\n",
      "Target 9: Lending\n",
      "Choices:\n",
      "founders\n",
      "fashion\n",
      "those\n",
      "lending\n",
      "body\n",
      "alternative\n",
      "feel\n",
      "Lending\n",
      "minimalist\n",
      "happening\n"
     ]
    }
   ],
   "source": [
    "# Make sure that all options include the correct answer\n",
    "for i in range(len(df_choices)):\n",
    "    target_i = df_choices['target'].iloc[i]\n",
    "    choices = df_choices['choices'].iloc[i]\n",
    "    print(f'\\nTarget {i}: {target_i}')\n",
    "    print('Choices:')\n",
    "    for j in choices:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4: Human Evaluation as Gold Standard\n",
    "\n",
    "(In Report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
